% -*- root: main.tex -*-

\begin{figure*}[thb!]
  \centering
  \includegraphics[width=1\textwidth]{figures/automatically_classified_data_approach.pdf}
  \caption{Automatically Classified Data Approach Overview}
  \label{fig:automatically_classified_data_approach_overview}
\end{figure*}

The main goal of our study is to understand what happens with \SATD comments after they get introduced into projects, and most specifically, who are the authors responsible for the introduction and removal of these comments. To do that, we divided our study in two main parts. First, we developed a tool that allow us to extract and classify \SATD comments based on NLP techniques. Using this tool we extracted and classified the comments of five open source projects. Second, we analyze the performance of our tool in terms of precision and recall using two other open source projects which we have manually classified the \SATD comments. 

In a nutshell the tool first clones the analyzed repository locally. Second, it identifies all Java source code files in the repository. By analyzing the history of the project we are able to identify even files that were already removed. Third, we use a third part library to parse the source code and extract the information that we use in our analysis. Fourth, we apply four filtering heuristics that aims to eliminate non \SATD comments. Fifth, it uses NLP techniques to classify the comments as \SATD. Lastly, the tool identifies the authors that are responsible for the introduction and removal of the \SATD comments. 

Figure~\ref{fig:automatically_classified_data_approach_overview} shows an overview of our approach, and the following subsections detail each step.

\subsection{Project Data Extraction}
\label{sub:project_data_extraction}

The main key to perform our study is source code comments. Based on the comments found in the source code files we are able to identify and later analyze \SATD. We decided to extract the source code comments of five open source projects, namely Camel, Gerrit, Hadoop, Log4j and Tomcat. We select these projects since they belong to different application domains, are well commented, vary in size, number of contributors, and with a high activity level. Additionally, we selected projects that have git as their source code repository.

Table ~\ref{tab:project_details} provide details about each of the projects used in our study. The columns of Table \ref{tab:project_details} present the number of Java files analyzed, followed by the total source lines of code (SLOC), the number of different file versions that we find analyzing the history of the project, the number of contributors, the number of extracted comments, the number of comments analyzed after applying our filtering heuristics, the number of comments that were classified as \SATD and finally the number of comments unique \SATD comments. To calculate the unique \SATD number we take in consideration only the first time that the comment appears on any of the different file versions. 

Since there are many different definitions for the SLOC metric we clarify that, in our study, a source line of code contains at least one valid character, which is not a blank space or a source code comment. In addition, we only use the Java files to calculate the SLOC, and to do so, we use the SLOCCount tool~\cite{wheeler2004:home}. 

The number of contributors and the level of activity was extracted from OpenHub, an on-line community and public directory that offers analytics, search services and tools for open source software \cite{Openhub:home}. Activity level is a metric provided by OpenHub, and it is based on a combination of contributor count and commit count, with more weight placed on contributors. These two metrics are combined so that recent activity is much more heavily weighted, i.e., a contribution this past month is far more important in determining activity level than a contribution a year ago \cite{Openhub:activity_level}. 

It is important to note that the number of comments shown for each project does not represent the number of commented lines, but rather the number of Single-line, Block and Javadoc comments. In total, we obtained 7,749,969 comments, found in 30,915 Java classes, and its 446,775 different versions. The size of the selected projects varies between 30,287 and 800,488 SLOC, and the number of contributors of these projects ranges from 32 to 289. 

\begin{table*}[thb!]
    \begin{center}
    \caption{Details of Studied Projects}
    \label{tab:project_details}
    
            \begin{tabular}{l| c c r c || c c c c }
            \toprule
            
            \multirow{5}{*}{\textbf{\thead{Project}}} & \multicolumn{4}{c||}{\textbf{\thead{Project details}}} & \multicolumn{4}{c}{\textbf{\thead{Comments details}}} 

            \\
            \cmidrule{2-9}

            & \textbf{\thead{\# of files}} & \textbf{\thead{SLOC}} & \textbf{\thead{\# of file\\versions}} & \textbf{\thead{\# of \\contributors}}  & \textbf{\thead{\# of \\comments}}   & \textbf{\thead{\# of \\comments \\after filtering}} & \textbf{\thead{\# of \\TD \\comments}}  & \textbf{\thead{\# of \\unique TD \\comments}}\\ 
            \midrule 
            \textbf{Camel}     & 15,091 & 800,488 & 254,920 & 289  &  1,634,361 &   700,412  &  20,141 &  4,331   \\
            \textbf{Gerrit}    &  3,059 & 222,476 &  53,298 & 270  &  1,018,006 &   129,023  &   4,810 &   271    \\
            \textbf{Hadoop}    &  8,466 & 996,877 &  79,232 & 160  &  2,512,673 &  1,172,051 &  18,927 &  1,164   \\
            \textbf{Log4j}     &  1,112 & 30,287  &  12,609 & 35   &    248,276 &    61,690  &   1,893 &   135    \\
            \textbf{Tomcat}    &  3,187 & 297,828 &  46,716 & 32   &  2,336,653 &  1,081,492 &  26,725 &  1,317   \\
            \bottomrule             
        \end{tabular}
    \end{center}
\end{table*}

\subsection{Checkout All Versions of Files}
\label{sub:checkout_all_versions_of_files}

The life cycle of a software project is very dynamic and it comprehends a lot of changes. New Features are being introduced while old ones are being maintained and sometimes removed. The same happens with source code comments, which in consequence means that \SATD comments are being removed and introduced during the evolution of the software project. Therefore, analyzing a single version of a project is not enough to study the behavior of \SATD comments. In our study we take the ever changing nature of software projects into account, and we use our approach to identify \SATD comments independently in all different file versions of the project.

To achieve that, we first identify all Java source code files currently available in the latest version of the project. Then, we analyze the source code repository to track all changes done to each file. Each change made to a file produces a different version of that file, and by extracting them we can analyze each file version looking for \SATD. Git are capable of tracking  renamed or moved files based on a similarity threshold. In our study, we use this similarity threshold at 90\%. For example, if a file is renamed or moved to another folder, and it still remains at least 90\% similar with its older version, Git will consider that the file was just moved or renamed. However, if after the change the file is less than 90\% similar with the previous version Git will consider that the original file was removed and a new one was created instead. Using this feature from Git, we are able to trace and extract all versions of the Java source files that are currently in the repository. 

The second step to checkout all versions of files is to identify the files that are not present in the repository anymore (i.e., deleted files). To do so, we again rely on a feature from Git that shows us all the files that has been removed from the repository. By parsing the output that Git provides us we can obtain the commit hash and the fully qualified name of the file. Using this information we extract the deleted file and repeat the process described above to obtain all the older versions of this file. To guarantee the correctness of our process while parsing the Git output for deleted files we make sure that we are looking for only Java source files, and that the fully qualified path of the file that we are extracting is not already in stored in our database. 

Once that this step is over, we have at our disposal the information regarding the files and its different versions stored in the database, and an actual copy of each file version in a structured directory inside our tool that will be used to extract the remainder data for our study.

\subsubsection*{Parse Source Code}
\label{subsub:parse_source_code}

We use SrcML \cite{srcml} to parse the source code files that we identified in the repository. SrcML is a open source library that parses source code files into a XML structure. The resulting XML file has specifics tags that allow us to easily identify elements that we want to extract. This way we extract all comments from the source code files and information related to them for example, the line that each comment starts, finishes and the type of the comment (i.e., Javadoc, Line or Block).

\subsubsection*{Filtering Comments}
\label{subsub:filtering_comments}

Comments are found in most of software projects.

License comments are not very likely to contain self-admitted technical debt, and are commonly added before the declaration of the class. We create a heuristic that removes comments that are placed before the class declaration. Since we know the line number that the class was declared we can easily check for comments that are placed before that line and remove them. In order to decrease the chances of removing a self-admitted technical debt comment while executing this filter we calibrated this heuristic to not remove comments containing one of task annotations (i.e., ``TODO:'', ``FIXME:'', or ``XXX:'') ~\cite{Storey2008ICSE}. Task annotations are an extended functionality provided by most of the popular Java \textit{IDEs} including Eclipse, InteliJ and NetBeans. When one of these words are used inside a comment the IDE will automatically keep track of the comment creating a centralized list of tasks that can be conveniently accessed later on.

Long comments that are created using multiple \emph{Single-line} comments instead of a \emph{Block} comment can hinder the understanding of the message considering the case that the reader (i.e., human or machine) analyzes each one of these comments independently. To solve that problem, we create a heuristic that searches for consecutive Single-line comments and groups them as one comment.
 
Commented source code is found in the projects due to many different reasons. One of the possibilities is that the code is not currently being used. Other is that, the code is used for debugging purposes only. Based on our analysis, commented source code does not have self-admitted technical debt. Our heuristic removes commented source code using a simple regular expression that captures typical Java code structures.

Automatically generated comments by the IDE are filtered out as well. These comments are inserted as part of code snippets used to generate constructors, methods and try catch blocks, and have a fixed format (i.e., ``Auto-generated constructor stub'', ``Auto-generated method stub'', and ``Auto-generated catch block''). Therefore our heuristic searches for these automatically generated comments and removes them. 

Javadoc comments rarely mention self-admitted technical debt. For the Javadoc comments that do mention self-admitted technical debt, we notice that they usually contain one of the task annotations (i.e., ``TODO:'', ``FIXME:'', or ``XXX:''). Therefore, our heuristic removes all comments of the type Javadoc unless they contain at least one of the task annotations  To do so, we create a simple regular expression that searches for the task annotations before removing the comment.  

The steps mentioned above significantly reduced the number of comments in our dataset and helped us focus on the most applicable and insightful comments. For example, in the Ant project, applying the above steps helped to reduce the number of comments from 21,587 to 4,137 meaning a reduction of 80.83\% in the number of comments to be manually analyzed. Using the filtering heuristics we were able to remove from 39.25\% to 85.89\% of all comments. Table \ref{tab:project_details} provides the number of comments kept after the filtering heuristics for each project.

\subsubsection*{NLP Classification}
\label{subsub:nlp_classification}
\todo{falar que procuramos debto tec en todos os arquivos}

\subsubsection*{Find Technical Debt Authors}
\label{subsub:find_technical_debt_authors}

Once we have classified all comments as \SATD or non \SATD we can search for the authors responsible for the introduction and eventual removal of the \SATD comments. As described in subsection \ref{subsub:checkout_all_versions_of_files} we have stored all different versions of all source code files. Therefore, to find the author who introduced a \SATD comment we look into the oldest version of the file containing the analyzed comment, and incrementally search through all future versions of that file until we find the first time that the comment appears for the first time. Once found, we keep tracking the \SATD comment in the remaining future versions of the file checking if  the \SATD comment was removed. 

Additionally, when searching for authors who removed \SATD comments we have to take into consideration deleted files. As we also identify the files that once was removed we can check if the last version of the file before removal contained \SATD comments. If that is the case, the author who removed the file is also the author who removed the \SATD comments. 

























\subsection*{Manually Classified Data Approach}
\label{sub:manually_classified_data_approach}

As shown in previous work, technical debt can be classified into different types ~\cite{Alves2014MTD}. However, design technical debt is the most common ~\cite{Maldonado2015MTD} and impactful ~\cite{Ernst2015FSE} type of debt. Therefore, to perform our study, we use manually classified \SATD comments from three different projects, namely Apache Ant, Apache Jmeter and Jruby. The analyzed dataset consists of 754 \SATD design comments distributed between the three projects. We choose to analyze Apache Ant, Apache Jmeter and Jruby as the version that contains the manually classified comments has enough past and future versions to be analyzed. Moreover, they have git repositories that are currently maintained enabling us to apply our approach. 

The manually classified comments are part of a bigger dataset of \SATD comments created during ours previous studies ~\cite{Maldonado2015MTD,Maldonado2015TSE}. Basically, during these previous works, we created a public available dataset containing 62,566 comments extracted from ten open source projects. These comments were classified as \SATD comments or as regular comments (i.e., comments without technical debt). The dataset was classified by the first author and later, to mitigate the risk of bias, another student was asked to classify a statistically significant sample of the dataset. The Cohen's kappa coefficient ~\cite{cohen1960coefficient} (i.e., the level of agreement between both reviewer) was of +0.81. The resulting coefficient is scaled to range between -1 and +1, where negative value means poorer than chance agreement, zero indicates exactly chance agreement, and positive value indicates better than chance agreement ~\cite{fleiss1973equivalence}.

\subsubsection*{Technical Debt Files Identification}
\label{subsub:technical_debt_files_identification}

We use the fully qualified name of the file (i.e., file path plus file name) to mine information from the source code repository, and the manually classified dataset contains the fully qualified name of each file that contains at least one \SATD design debt comment. However, the fully qualified name of the manually dataset does not correspond to the latest available version, and there is the possibility that the file has been moved around in future versions. The same problem could happen in past versions as well. Therefore, we need to identify the fully qualified name of each file on each future and past versions. 

In order to do that, we first extract the all versions of an project. Second, we incrementally try to match the fully qualified name in these versions. Every time that we have a match means that the file has not been moved. When we are not able to match the fully qualified name of the file we try to match just the file name. In the case that we have a single match using the file name we consider that the file has simply been moved to a different folder. In the case that we have more than one match using the file name we compare the similarity between the files (i.e., shortest added distance) and consider the most similar of the files as the file that has been moved. Lastly in the case that all these methods fails to find a match we consider that the file has been removed in the analyzed version.  

This process results in a mapping containing the fully qualified name for each version, and we store this information back in the database for future use.

\subsubsection*{Checkout All Versions of Technical Debt Files}
\label{subsub:checkout_all_versions_of_technical_debt_files}

Once that we identified the fully qualified name of the \SATD files we can extract from the source code repository all the different versions of these files. Having all versions of the \SATD files allow us to pinpoint the exact moment that the \SATD comment was introduced and if it was later removed.

First, we group the different fully qualified names that we obtained for each file. For example, if ``file1'' was in ``path1'' during 10 versions and then it was moved to ``path2''. The group of fully qualified names for ``file1'' would have two items: ``path1/file1'' and ``path2/file1''. 

Second, we identify the latest version (i.e., tag) that corresponding to each fully qualified name. This way we can checkout the repository to the desired version and extract all the changes (i.e., commits) done to the file considering the fully qualified name. This information is parsed and inserted into our database, providing us quick access to all commits that touched each \SATD file. 

Third, we use the commit list of each \SATD file to checkout all changed versions. This computation results in a collection of files that represents the full history of the \SATD file.  

\subsubsection*{Identify Author Who Introduced the Technical Debt}
\label{subsub:identify_author_who_introduced_the_technical_debt}

The next step is to identify the author who introduced the technical debt. In order to do so, we search for each \SATD comment that was manually classified in all different file versions that it belongs. We start by analyzing the oldest version of the \SATD file looking for a match. In the case that the match is negative we move to the next \SATD file version. We repeat this process until we find the exact match of the \SATD comment. 

We consider that we found the introduction of the \SATD comment when we have a exact match of the manual classified \SATD comment. In some cases the \SATD comment possess multiple lines, and it is possible to this comment to have partial matches (i.e., not all lines) across the files. However, we consider that the manual classified \SATD comment offers us the certainty that the comment is in fact a \SATD and should be considered in full while searching the version that it was introduced.

\subsubsection*{Identify Author Who Removed the Technical Debt}
\label{subsub:identify_author_who_removed_the_technical_debt}

To identify the author who removed the technical debt we start by analyzing the next file version that the \SATD comment was found. We incrementally check all future versions of the file until we can not find a match anymore. In the case that we are able to find the \SATD comment in all future versions means that the \SATD comment was not removed, and therefore has not a removed author to be identified. 

It is also possible that during the development of the project the \SATD file has been removed. As we have the mapping between the fully qualified name of the file and the version that it belongs we can also identify if they were removed. When we identify that the file was removed, we assume that the author responsible for the deletion is also the author who removed the \SATD comments that were in that file.



% \subsection*{Automatically Classified Data Approach}
% \label{sub:automatically_classified_data_approach}

% We developed a Python based tool that is capable of mining all the necessary information for our study based on a git repository URL. In a nutshell the tool first clones the analyzed repository locally. Second, it identifies all Java source code files in the repository. By analyzing the history of the project the tool identifies even the files that were already removed. Third, we use a third part library to parse the source code and extract the information that we use in our analysis. Fourth, we apply filtering heuristics that aims to eliminate non \SATD comments. Fifth, it uses NLP techniques to classify the comments as \SATD. Lastly, the tool identifies the authors that are responsible for the introduction and removal of the \SATD comments. 

% In an effort to evaluate the performance of our tool we compare the \SATD classification obtained doing this process with the manually classified dataset. 

\subsubsection*{Project Data Extraction}
\label{subsub:project_data_extraction}

To perform our study we selected five open source projects namely Camel, Gerrit, Hadoop, Log4j and Tomcat. These projects belongs to different application domains, and accordingly with OpenHub, they are well commented projects  with high level of developer activity. OpenHub is an on-line community and public directory that offers analytics, search services and tools for open source software \cite{Openhub:home}.

\subsubsection*{Checkout All Versions of Files}
\label{subsub:checkout_all_versions_of_files}

\subsubsection*{Parse Source Code}
\label{subsub:parse_source_code}

We use SrcML \cite{srcml} to parse the source code files that we identified in the repository. SrcML is a open source library that parses source code files into a XML structure. The resulting XML file has specifics tags that allow us to easily identify elements that we want to extract. This way we extract all comments from the source code files and information related to them for example, the line that each comment starts, finishes and the type of the comment (i.e., Javadoc, Line or Block).

\subsubsection*{Filtering Comments}
\label{subsub:filtering_comments}

Comments are found in most of software projects.

License comments are not very likely to contain self-admitted technical debt, and are commonly added before the declaration of the class. We create a heuristic that removes comments that are placed before the class declaration. Since we know the line number that the class was declared we can easily check for comments that are placed before that line and remove them. In order to decrease the chances of removing a self-admitted technical debt comment while executing this filter we calibrated this heuristic to not remove comments containing one of task annotations (i.e., ``TODO:'', ``FIXME:'', or ``XXX:'') ~\cite{Storey2008ICSE}. Task annotations are an extended functionality provided by most of the popular Java \textit{IDEs} including Eclipse, InteliJ and NetBeans. When one of these words are used inside a comment the IDE will automatically keep track of the comment creating a centralized list of tasks that can be conveniently accessed later on.

Long comments that are created using multiple \emph{Single-line} comments instead of a \emph{Block} comment can hinder the understanding of the message considering the case that the reader (i.e., human or machine) analyzes each one of these comments independently. To solve that problem, we create a heuristic that searches for consecutive Single-line comments and groups them as one comment.
 
Commented source code is found in the projects due to many different reasons. One of the possibilities is that the code is not currently being used. Other is that, the code is used for debugging purposes only. Based on our analysis, commented source code does not have self-admitted technical debt. Our heuristic removes commented source code using a simple regular expression that captures typical Java code structures.

Automatically generated comments by the IDE are filtered out as well. These comments are inserted as part of code snippets used to generate constructors, methods and try catch blocks, and have a fixed format (i.e., ``Auto-generated constructor stub'', ``Auto-generated method stub'', and ``Auto-generated catch block''). Therefore our heuristic searches for these automatically generated comments and removes them. 

Javadoc comments rarely mention self-admitted technical debt. For the Javadoc comments that do mention self-admitted technical debt, we notice that they usually contain one of the task annotations (i.e., ``TODO:'', ``FIXME:'', or ``XXX:''). Therefore, our heuristic removes all comments of the type Javadoc unless they contain at least one of the task annotations  To do so, we create a simple regular expression that searches for the task annotations before removing the comment.  

The steps mentioned above significantly reduced the number of comments in our dataset and helped us focus on the most applicable and insightful comments. For example, in the Ant project, applying the above steps helped to reduce the number of comments from 21,587 to 4,137 meaning a reduction of 80.83\% in the number of comments to be manually analyzed. Using the filtering heuristics we were able to remove from 39.25\% to 85.89\% of all comments. Table \ref{tab:project_details} provides the number of comments kept after the filtering heuristics for each project.

\subsubsection*{NLP Classification}
\label{subsub:nlp_classification}
\todo{falar que procuramos debto tec en todos os arquivos}

\subsubsection*{Find Technical Debt Authors}
\label{subsub:find_technical_debt_authors}

Once we have classified all comments as \SATD or non \SATD we can search for the authors responsible for the introduction and eventual removal of the \SATD comments. As described in subsection \ref{subsub:checkout_all_versions_of_files} we have stored all different versions of all source code files. Therefore, to find the author who introduced a \SATD comment we look into the oldest version of the file containing the analyzed comment, and incrementally search through all future versions of that file until we find the first time that the comment appears for the first time. Once found, we keep tracking the \SATD comment in the remaining future versions of the file checking if  the \SATD comment was removed. 

Additionally, when searching for authors who removed \SATD comments we have to take into consideration deleted files. As we also identify the files that once was removed we can check if the last version of the file before removal contained \SATD comments. If that is the case, the author who removed the file is also the author who removed the \SATD comments. 