% -*- root: main.tex -*-

The main goal of our study is to understand the removal of \SATD.  Although some prior work examined the removal of \SATD and who removes \SATD (e.g.,~\cite{Potdar2014ICSME,Bavota2016MSR}), to the best of our knowledge, this is one of the first studies to \emph{exclusively} focus on the removal of \SATD. Furthermore, in addition to quantifying removal and examining who removes \SATD, we also examine how long \SATD tends to live in a project and shed light on the activities that lead to the removal of \SATD. Also, our study uses a more accurate way to determine \SATD and examines a different set of projects than the aforementioned work, strengthening the empirical evidence of \SATD removal.



To conduct our study, we checkout all versions of five large, well-commented, open source projects. Then, we use the NLP technique recently presented by Maldonado \emph{et al.}~\cite{Maldonado2015TSE} to detect \SATD based on source code comments. Once \SATD has been identified, we can also conclude when and by whom it was introduced and removed.
Figure~\ref{fig:automatically_classified_data_approach_overview} shows an overview of our approach, and the following subsections detail each of its steps.


\begin{figure}[tb!]
	\centering
	\includegraphics[width=\columnwidth]{figures/approach.pdf}
	\caption{Automatic classification of \SATD.}
	\label{fig:automatically_classified_data_approach_overview}
\end{figure}

\subsection{Project Data Extraction}
\label{sub:project_data_extraction}
We start by selecting case study projects. While sometimes even a single case might be sufficient, \emph{e.g.}, when it is typical, we study several projects as multiple case design is known to usually offer greater validity~\cite{Easterbrook2008}.
We select projects to cover different application domains, sizes and numbers of contributors. Furthermore, since the \SATD identification heavily depends on source code comments, we selected \emph{well-commented} projects, and since we are interested in changes in \SATD (introduction and removal) we focus on highly active projects. 
All projects are developed in Java and use Git. We selected five open source projects namely Camel, Gerrit, Hadoop, Log4j, and Tomcat, and started analyzing the selected projects on March 15th 2016.



Table~\ref{tab:project_details} provides details about each of the projects used in our study. The columns of Table~\ref{tab:project_details} include the number of extracted comments (\textit{i.e.,} from all versions), the number of comments analyzed after applying our filtering heuristics (\textit{i.e.,} removing commented-out source code, license comments and Javadoc comments), the number of comments that were classified as \SATD and finally the number of unique \SATD comments. To calculate the number of unique \SATD comments, we take in consideration only the first time that the comment appears on any of the different file versions. This is necessary because the same comment may appear in different versions of the file. In total, we obtained 7,749,969 comments, found in 446,775 different versions of 30,915 Java classes. The size of the selected projects varies between 30,287 and 800,488 SLOC, and the number of contributors of these projects ranges from 32 to 289. Since there are exist different definitions for the SLOC metric we clarify that, in our study, a source line of code contains at least one valid character, which is not a blank space or a source code comment. In addition, we only use the Java files to calculate the SLOC, and to do so, we use the SLOCCount tool~\cite{wheeler2004:home}. 

 
The number of contributors was extracted from OpenHub, an on-line community and public directory that offers analytics, search services and tools for open source software \cite{Openhub:home}. Number of contributors is calculated by counting the different authors that committed changes to the source code repository. However, there is the possibility that one developer possesses more than one user name in the source code repository~\cite{DBLP:conf/icsm/KoutersVSB12}. To mitigate this risk, OpenHub provides an interface where the manager of the project on OpenHub can link two or more different user names belonging to the same user~\cite{Openhub:Aliases}. 



The number of comments shown in Table~\ref{tab:project_details} for each project does not represent the number of commented lines, but rather the number of Line, Block and Javadoc comments. 



\begin{table*}[thb!]
    \begin{center}
    \caption{\textsc{Details of studied projects.}}
    \label{tab:project_details}
             \begin{tabular}{l|rrrr||rrrr}
            \toprule
             \multirow{5}{*}{\textbf{\thead{Project}}} & \multicolumn{4}{c||}{\textbf{\thead{Project details}}} & \multicolumn{4}{c}{\textbf{\thead{Comments details}}} 
            \\
            \cmidrule{2-9}
            & \textbf{\thead{\# Java\\ files}} & \textbf{\thead{SLOC}} & \textbf{\thead{\# file\\versions}} & \textbf{\thead{\# contributors}}  & \textbf{\thead{\# comments}}   & \textbf{\thead{\# comments \\after filtering}} & \textbf{\thead{\# TD \\comments}}  & \textbf{\thead{\# unique TD \\comments}}\\ 
            \midrule 
            \textbf{Camel}     & 15,091 & 800,488 & 254,920 & 289  &  1,634,361 &   700,412  &  20,141 &  4,331   \\
            \textbf{Gerrit}    &  3,059 & 222,476 &  53,298 & 270  &  1,018,006 &   129,023  &   4,810 &   271    \\
            \textbf{Hadoop}    &  8,466 & 996,877 &  79,232 & 160  &  2,512,673 &  1,172,051 &  18,927 &  1,164   \\
            \textbf{Log4j}     &  1,112 & 30,287  &  12,609 & 35   &    248,276 &    61,690  &   1,893 &   135    \\
            \textbf{Tomcat}    &  3,187 & 297,828 &  46,716 & 32   &  2,336,653 &  1,081,492 &  26,725 &  1,317   \\  
            \bottomrule             
        \end{tabular}
    \end{center}
\end{table*}


\subsection{Checkout All Versions of Files}
\label{sub:checkout_all_versions_of_files}


\begin{figure}[tb!]
	\centering
	\includegraphics[width=\columnwidth]{figures/test/files_cases.pdf}
	\caption{Different cases when checking out all file versions.}
	\label{fig:files_case}
\end{figure}



Since we focus on the introduction and removal of \SATD, historical information about the project files is sought. As illustrated in Figure~\ref{fig:files_case}, we first identify all Java source code files currently available in the latest version of the project. Then, we analyze the source code repository to track all changes done to each file. Each change made to a file produces a different version of that file (Figure~\ref{fig:files_case} case 1), and by extracting them we can analyze each file version looking for source comments that indicate \SATD. A version of a file is generated whenever a change (i.e., commit) is made. Once we identify all file versions, we consider the first available file version that contains the \SATD as the file version that introduced the \SATD. Similarly, we consider the date of the commit that removes the \SATD comment or deletes the file where the \SATD exists in as the removal date.

Since file deletions indicate \SATD removals, it is important to correctly handle file moves and renames (Figure~\ref{fig:files_case} case 2). Git is capable of tracking renamed or moved files based on a similarity threshold~\cite{BirdMSR2009,HataIWPSE-EVOL2011}. In our study, we use the default similarity threshold of 90\%, \emph{i.e.}, if a file is renamed or moved to another folder, and is at least 90\% similar to an older version (excluding whitespaces and blank lines), Git will consider that the file was just moved or renamed. If a file was not listed as being moved or renamed, then we consider it to be deleted. 





The second step to checkout all versions of files is to identify the files that are no longer present in the repository  (\textit{i.e.,} deleted files,(Figure~\ref{fig:files_case} case 3). Using Git we obtain the list of commit hashes that have deleted at least one file and the fully qualified names of these files. Using this information, we repeat the process described above to obtain all the older versions of the files. To guarantee the correctness of the process we focus solely on Java source files, and ensure that every fully qualified path of the file is analyzed only once. 



After this step is complete, we have at our disposal the information regarding the files and their versions stored in the database. We also store an actual copy of each file version in a structured directory that we will use to extract the remainder of the data for our study.

\subsection{Extracting Source Code Comments}
\label{sub:parse_source_code}

We use an open source library SrcML~\cite{Collard2013SIE} to parse the source code, and extract the comments and the information related to them such as the line that each comment starts, finishes and the type of the comment (\textit{i.e.,} Javadoc, Line or Block). 


As the prior work showed, not all comments can contain \SATD \cite{Potdar2014ICSME,Wehaibi2016SANER}. Therefore,  as the pre-processing we exclude the following types of comments:

\begin{itemize}
\item \textbf{License comments} that generally do not contain \SATD and are commonly located before the class declaration. That said, comments that contain task annotations (\textit{i.e.,} ``TODO:'', ``FIXME:'', or ``XXX:'') ~\cite{Storey2008ICSE} are not removed since they are usually leveraged by most IDEs, e.g., Eclipse and Netbeans, to automatically generate task lists.

%We create a heuristic that removes comments that are placed before the class declaration. Since we know the line number that the class was declared we can easily check for comments that are placed before that line and remove them. In order to decrease the chances of removing a self-admitted technical debt comment while executing this filter we calibrated this heuristic to not remove comments containing one of task annotations (\textit{i.e.,} ``TODO:'', ``FIXME:'', or ``XXX:'') ~\cite{Storey2008ICSE}. Task annotations are an extended functionality provided by most of the popular Java \textit{IDEs} including Eclipse, InteliJ and NetBeans. When one of these words are used inside a comment the IDE will automatically keep track of the comment creating a centralized list of tasks that can be conveniently accessed later on. \alexander{I miss here something like ``We consider task annotations as \SATD.'' Or ``Following ... we consider...''}

%\item \textbf{Long comments} that are created using multiple \emph{Single-line} comments instead of a \emph{Block} comment can hinder the understanding of the message considering the case that the reader (\textit{i.e.,} human or machine) analyzes each one of these comments independently. To solve that problem, we create a heuristic that searches for consecutive Single-line comments and groups them as one \alexander{Block?} comment.\everton{Block could be confusing because it is a specific entity of the Java language with its own syntax. What we have here is more like an group of single line comments. But, if you think that block makes the text clearer to the reader I am all in.}

\item \textbf{\rabe{Commented-out} source code} \rabe{by developers 
	which is not compiled,} is also ignored since prior work showed that it generally does not contain \SATD \cite{Potdar2014ICSME,Wehaibi2016SANER}.
% found in the projects due to many different reasons. One of the possibilities is that the code is not currently being used. Other is that, the code is used for debugging purposes only. Based on our analysis, commented source code does not have self-admitted technical debt.\alexander{``Our analysis'' is very vague here: what has been analyzed and how? What kind of results did we obtain?} \everton{I agree that analysis here is not ideal. This was part of one of our first studies when I joined Concordia. It was a course project, and basically, after we extracted source code comments from ten projects (same projects from TSE) we manually read through them to evaluate the results. Some patterns, became very clear. For example, all classes from one project had license comments. These license comments does not contains TD , so we removed. That is how we created our heuristics}Our heuristic removes commented source code using a simple regular expression that captures typical Java code structures.

\item \textbf{Automatically generated comments} by the IDE are also removed since they, by definition, do not indicate \SATD. 
%These comments are inserted as part of code snippets used to generate constructors, methods and try catch blocks, and have a fixed format (\textit{i.e.,} ``Auto-generated constructor stub'', ``Auto-generated method stub'', and ``Auto-generated catch block'' \alexander{is this an extensive list or are these merely examples?}).\everton{An extensive list.} Therefore, our heuristic searches for these automatically generated comments and removes them. 

\item \textbf{Javadoc comments} are also removed since they rarely mention self-admitted technical debt \cite{Potdar2014ICSME}. 
%For the Javadoc comments that do mention self-admitted technical debt, we notice that they usually\alexander{how often?} \everton{Not often at all, but not quantified yet. To quantify, query the number of comments from table processed\_comment where type = Javadoc and classification is different than WITHOUT\_CLASSIFICATION.} contain one of the task annotations (\textit{i.e.,} ``TODO:'', ``FIXME:'', or ``XXX:''). Therefore, our heuristic removes all comments of the type Javadoc unless they contain at least one of the task annotations  To do so, we create a simple regular expression that searches for the task annotations before removing the comment.  
 
\end{itemize} 


The pre-processing steps above significantly reduce the number of comments in the dataset and allow us to focus on the most applicable and insightful comments. For example, as shown in Table~\ref{tab:project_details}, in the Camel project, applying the above steps helped to reduce the number of comments from 1,634,361 to 700,412 meaning a reduction of 57.1\% in the number of comments to be classified. Using the filtering heuristics we were able to eliminate between 53.3\% to 87.3\% of all comments. Table \ref{tab:project_details} provides the number of comments kept after the filtering heuristics for each project.
%\rabe{I could not figure out from where these comment are calculated, is it from the current version of all files, in all version of files}
%\alexander{how did we ensure that these were the most ``applicable and insghtful''?} \everton{This statement was intuitively made. Here we are not trying to say that the removed comments were not important, we are trying to say that from our perspective (that is classifying TD comments) removing noisy comments (comments that does not have TD) leaves us with the most applicable comments (for the task)} 

%\subsection{Filtering Comments}
%\label{sub:filtering_comments}

%Source code comments can be used for different purposes in a project, such as giving context, documenting, expressing thoughts, opinions and authorship, and in some cases, disabling source code from the program.\alexander{This is a nice statement but it somehow seems to call for a bibliographic reference.} \rabe{not sure about this \cite{Ying2005MSR}} \everton{This statement was me thinking of how I have used comments in the past. Nevertheless, the same statement now is published in the TSE. I do not know if this counts though.}Comments are used freely by developers and with limited formalities, if any at all. This informal environment allows developers to bring to light opinions, insights and even confessions (e.g., self-admitted technical debt). 

%Part of these comments can be identified as self-admitted technical debt \cite{Potdar2014ICSME}, \alexander{Would you consider the comments themselves as \SATD or merely as admissions, or indications that the technical debt is somewhere around the comment?} \everton{Mostly the second option. For example, Fluri et al. in ``Do code and comments co-evolve? on the relation between source code and comment changes'' states that 97\% of the comment changes are consistent (i.e., the code comment is changed with the related piece of code). However, for the remainder of the cases I would consider option 1. }but they are not the majority of cases. With that in mind, we develop and apply five filtering heuristics \alexander{explicitly number the heuristics below} to narrow down the comments eliminating the ones that are less likely to be classified as self-admitted technical debt. 




%\alexander{I miss here an evaluation of the approach. The fact that filtering reduces the number of comments is hardly surprising. Can you test these heuristics on a set of comments that you have not seen when designing the heuristics?}\rabe{the heuristics filtering could be considered as prepossessing step}\everton{The heuristics can be applied on any Java project. They can be used because they address common usage patterns of code comments in Java projects. The results, (number of comments filtered) will vary from project to project though.}

%\subsection{NLP Classification}
%\label{sub:nlp_classification}

\subsection{Applying the NLP to Identify Self-Admitted Technical Debt} 
To identify the \SATD comments, we next use the technique presented in by Maldonado \emph{et al.}~\cite{Maldonado2015TSE}. We refer readers to Maldonado \emph{et al.}'s paper for full details on how to identify \SATD comments, however, to make our paper self-contained, we highlight the key points of their approach that we use next.

\begin{itemize}
	\item To train the NLP classifier, we used the manually classified \SATD comments dataset provided by Maldonado \emph{et al.}~\cite{Maldonado2015TSE}. The dataset contains 62,566 comments extracted from ten open source projects. These comments were classified as \SATD comments or as regular comments (\textit{i.e.,} comments without \SATD). The manually classified dataset was verified by the authors of Maldonado \emph{et al.}'s paper and they showed that two independent reviewers agreed on the classification, achieving a Cohen's Kappa value of +0.81~\cite{Maldonado2015TSE}. Hence, we have good confidence in the dataset provided by Maldonado \emph{et al.} and used in this study.
	
	\item We use the Stanford NLP Classifier~\cite{Manning2014ACL} to classify \SATD comments.
	The NLP Classifier takes as input classified data items (comments), and automatically learns \textit{features} (\textit{i.e.,} words) from each \textit{datum} that are associated with positive or negative numeric \textit{votes} for each class. The weights of the features are learned automatically based on the manually classified training data items (supervised learning). The Stanford NLP Classifier builds a \textit{maximum entropy model}~\cite{nigam1999using}, which is equivalent to a multi-class regression model, and is trained to maximize the conditional likelihood of the classes taking into account feature dependences when calculating the feature weights.
	
	\item In addition, the work by Maldonado \emph{et al.}~\cite{Maldonado2015TSE} showed that the NLP classifier is correct in identifying \SATD with an average precision of 0.72 and recall of 0.56. Although these precision and recall values may not seem high, they do represent the state of the art and outperform the comment-patterns technique, which all prior work was built on top of (i.e., \cite{Wehaibi2016SANER,Bavota2016MSR,Potdar2014ICSME}) by 230\%, on average. 
\end{itemize}

Now that we have a trained NLP classifier (\textit{maximum entropy classifier}), we follow a general process of machine learning where we apply the trained NLP classifier to classify the extracted comments from our five studied projects. We are confident in the trained NLP classifier, which is trained on the data provided by Maldonado \emph{et al.}~\cite{Maldonado2015TSE}, since Maldonado \emph{et al.} showed that the trained NLP classifier is able to produce good results even when it is evaluated on cross-project data. We discuss the impact of using the NLP classifier in more details in Section~\ref{sec:threats_to_validity}. The last two columns of Table~\ref{tab:project_details} show the number of identified \SATD comments per project after applying the trained NLP classifier.
