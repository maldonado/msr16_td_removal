% -*- root: main.tex -*-

\begin{figure*}[thb!]
  \centering
  \includegraphics[width=1\textwidth]{figures/automatically_classified_data_approach.pdf}
  \caption{Automatically Classified Data Approach Overview}
  \label{fig:automatically_classified_data_approach_overview}
\end{figure*}

The main goal of our study is to understand what happens with \SATD comments after they get introduced into projects, and most specifically, who are the authors responsible for the introduction and removal of these comments. To do that, we divided our study in two main parts. First, we developed a tool that allow us to extract and classify \SATD comments based on NLP techniques. Using this tool we extracted and classified the comments of five open source projects. Second, we analyze the performance of our tool in terms of precision and recall using two other open source projects which we have manually classified the \SATD comments. 

In a nutshell the tool first clones the analyzed repository locally. Second, it identifies all Java source code files in the repository. By analyzing the history of the project we are able to identify even files that were already removed. Third, we use a third part library to parse the source code and extract the information that we use in our analysis. Fourth, we apply four filtering heuristics that aims to eliminate non \SATD comments. Fifth, it uses NLP techniques to classify the comments as \SATD. Lastly, the tool identifies the authors that are responsible for the introduction and removal of the \SATD comments. 

Figure~\ref{fig:automatically_classified_data_approach_overview} shows an overview of our approach, and the following subsections detail each step.

\subsection{Project Data Extraction}
\label{sub:project_data_extraction}

The main key to perform our study is source code comments. Based on the comments found in the source code files we are able to identify and later analyze \SATD. We decided to extract the source code comments of five open source projects, namely Camel, Gerrit, Hadoop, Log4j and Tomcat. We select these projects since they belong to different application domains, are well commented, vary in size, number of contributors, and with a high activity level. Additionally, we selected projects that have git as their source code repository.

Table ~\ref{tab:project_details} provide details about each of the projects used in our study. The columns of Table \ref{tab:project_details} present the number of Java files analyzed, followed by the total source lines of code (SLOC), the number of different file versions that we find analyzing the history of the project, the number of contributors, the number of extracted comments, the number of comments analyzed after applying our filtering heuristics, the number of comments that were classified as \SATD and finally the number of comments unique \SATD comments. To calculate the unique \SATD number we take in consideration only the first time that the comment appears on any of the different file versions. 

Since there are many different definitions for the SLOC metric we clarify that, in our study, a source line of code contains at least one valid character, which is not a blank space or a source code comment. In addition, we only use the Java files to calculate the SLOC, and to do so, we use the SLOCCount tool~\cite{wheeler2004:home}. 

The number of contributors and the level of activity was extracted from OpenHub, an on-line community and public directory that offers analytics, search services and tools for open source software \cite{Openhub:home}. Activity level is a metric provided by OpenHub, and it is based on a combination of contributor count and commit count, with more weight placed on contributors. These two metrics are combined so that recent activity is much more heavily weighted, i.e., a contribution this past month is far more important in determining activity level than a contribution a year ago \cite{Openhub:activity_level}. 

It is important to note that the number of comments shown for each project does not represent the number of commented lines, but rather the number of Line, Block and Javadoc comments. In total, we obtained 7,749,969 comments, found in 30,915 Java classes, and its 446,775 different versions. The size of the selected projects varies between 30,287 and 800,488 SLOC, and the number of contributors of these projects ranges from 32 to 289. 

\begin{table*}[thb!]
    \begin{center}
    \caption{Details of Studied Projects}
    \label{tab:project_details}
    
            \begin{tabular}{l| c c r c || c c c c }
            \toprule
            
            \multirow{5}{*}{\textbf{\thead{Project}}} & \multicolumn{4}{c||}{\textbf{\thead{Project details}}} & \multicolumn{4}{c}{\textbf{\thead{Comments details}}} 

            \\
            \cmidrule{2-9}

            & \textbf{\thead{\# of files}} & \textbf{\thead{SLOC}} & \textbf{\thead{\# of file\\versions}} & \textbf{\thead{\# of \\contributors}}  & \textbf{\thead{\# of \\comments}}   & \textbf{\thead{\# of \\comments \\after filtering}} & \textbf{\thead{\# of \\TD \\comments}}  & \textbf{\thead{\# of \\unique TD \\comments}}\\ 
            \midrule 
            \textbf{Camel}     & 15,091 & 800,488 & 254,920 & 289  &  1,634,361 &   700,412  &  20,141 &  4,331   \\
            \textbf{Gerrit}    &  3,059 & 222,476 &  53,298 & 270  &  1,018,006 &   129,023  &   4,810 &   271    \\
            \textbf{Hadoop}    &  8,466 & 996,877 &  79,232 & 160  &  2,512,673 &  1,172,051 &  18,927 &  1,164   \\
            \textbf{Log4j}     &  1,112 & 30,287  &  12,609 & 35   &    248,276 &    61,690  &   1,893 &   135    \\
            \textbf{Tomcat}    &  3,187 & 297,828 &  46,716 & 32   &  2,336,653 &  1,081,492 &  26,725 &  1,317   \\
            \bottomrule             
        \end{tabular}
    \end{center}
\end{table*}

\subsection{Checkout All Versions of Files}
\label{sub:checkout_all_versions_of_files}

The life cycle of a software project is very dynamic and it comprehends a lot of changes. New Features are being introduced while old ones are being maintained and sometimes removed. The same happens with source code comments, which in consequence means that \SATD comments are being removed and introduced during the evolution of the software project. Therefore, analyzing a single version of a project is not enough to study the behavior of \SATD comments. In our study we take the ever changing nature of software projects into account, and we use our approach to identify \SATD comments independently in all different file versions of the project.

To achieve that, we first identify all Java source code files currently available in the latest version of the project. Then, we analyze the source code repository to track all changes done to each file. Each change made to a file produces a different version of that file, and by extracting them we can analyze each file version looking for \SATD. Git are capable of tracking  renamed or moved files based on a similarity threshold. In our study, we use this similarity threshold at 90\%. For example, if a file is renamed or moved to another folder, and it still remains at least 90\% similar with its older version, Git will consider that the file was just moved or renamed. However, if after the change the file is less than 90\% similar with the previous version Git will consider that the original file was removed and a new one was created instead. Using this feature from Git, we are able to trace and extract all versions of the Java source files that are currently in the repository. 

The second step to checkout all versions of files is to identify the files that are not present in the repository anymore (i.e., deleted files). To do so, we again rely on a feature from Git that shows us all the files that has been removed from the repository. By parsing the output that Git provides us we can obtain the commit hash and the fully qualified name of the file. Using this information we extract the deleted file and repeat the process described above to obtain all the older versions of this file. To guarantee the correctness of our process while parsing the Git output for deleted files we make sure that we are looking for only Java source files, and that the fully qualified path of the file that we are extracting is not already in stored in our database. 

Once that this step is over, we have at our disposal the information regarding the files and its different versions stored in the database, and an actual copy of each file version in a structured directory inside our tool that will be used to extract the remainder data for our study.

\subsection*{Parse Source Code}
\label{sub:parse_source_code}

We use SrcML \cite{Collard2013SIE} to parse the source code files that we identified in the repository. SrcML is a open source library that parses source code files into a XML structure. The resulting XML file has specifics tags that allow us to easily identify elements that we want to extract. This way we extract all comments from the source code files and information related to them for example, the line that each comment starts, finishes and the type of the comment (i.e., Javadoc, Line or Block). All the extracted information is stored in a relational database to facilitate the processing of the data. 


\subsection*{Filtering Comments}
\label{sub:filtering_comments}

Source code comments can be used for different purposes in a project, such as giving context, documenting, expressing thoughts, opinions and authorship, and in some cases, disabling source code from the program. Comments are used freely by developers and with limited formalities, if any at all. This informal environment allows developers to bring to light opinions, insights and even confessions (e.g., self-admitted technical debt). 

Part of these comments can be identified as self-admitted technical debt \cite{Potdar2014ICSME}, but they are not the majority of cases. With that in mind, we develop and apply 5 filtering heuristics to narrow down the comments eliminating the ones that are less likely to be classified as self-admitted technical debt. 

License comments are not very likely to contain self-admitted technical debt, and are commonly added before the declaration of the class. We create a heuristic that removes comments that are placed before the class declaration. Since we know the line number that the class was declared we can easily check for comments that are placed before that line and remove them. In order to decrease the chances of removing a self-admitted technical debt comment while executing this filter we calibrated this heuristic to not remove comments containing one of task annotations (i.e., ``TODO:'', ``FIXME:'', or ``XXX:'') ~\cite{Storey2008ICSE}. Task annotations are an extended functionality provided by most of the popular Java \textit{IDEs} including Eclipse, InteliJ and NetBeans. When one of these words are used inside a comment the IDE will automatically keep track of the comment creating a centralized list of tasks that can be conveniently accessed later on.

Long comments that are created using multiple \emph{Single-line} comments instead of a \emph{Block} comment can hinder the understanding of the message considering the case that the reader (i.e., human or machine) analyzes each one of these comments independently. To solve that problem, we create a heuristic that searches for consecutive Single-line comments and groups them as one comment.
 
Commented source code is found in the projects due to many different reasons. One of the possibilities is that the code is not currently being used. Other is that, the code is used for debugging purposes only. Based on our analysis, commented source code does not have self-admitted technical debt. Our heuristic removes commented source code using a simple regular expression that captures typical Java code structures.

Automatically generated comments by the IDE are filtered out as well. These comments are inserted as part of code snippets used to generate constructors, methods and try catch blocks, and have a fixed format (i.e., ``Auto-generated constructor stub'', ``Auto-generated method stub'', and ``Auto-generated catch block''). Therefore our heuristic searches for these automatically generated comments and removes them. 

Javadoc comments rarely mention self-admitted technical debt. For the Javadoc comments that do mention self-admitted technical debt, we notice that they usually contain one of the task annotations (i.e., ``TODO:'', ``FIXME:'', or ``XXX:''). Therefore, our heuristic removes all comments of the type Javadoc unless they contain at least one of the task annotations  To do so, we create a simple regular expression that searches for the task annotations before removing the comment.  

The steps mentioned above significantly reduced the number of comments in our dataset and helped us focus on the most applicable and insightful comments. For example, in the Camel project, applying the above steps helped to reduce the number of comments from 1,634,361 to 700,412 meaning a reduction of 57.1\% in the number of comments to be classified. Using the filtering heuristics we were able to remove from 53.3\% to 87.3\% of all comments. Table \ref{tab:project_details} provides the number of comments kept after the filtering heuristics for each project.

\subsection*{NLP Classification}
\label{sub:nlp_classification}

Our next step is to use the classified \SATD comments as a training dataset for the Stanford NLP Classifier~\cite{Manning2014ACL}.
A NLP Classifier, in general, takes as input a number of data items along with a classification for each data item, and automatically generates \textit{features} (i.e., words) from each \textit{datum}, which are associated with positive or negative numeric \textit{votes} for each class. The weights of the features are learned automatically based on the manually classified training data items (supervised learning). The NLP Classifier builds a \textit{maximum entropy model}, which is equivalent to a multi-class regression model, and it is trained to maximize the conditional likelihood of the classes taking into account feature dependences when calculating the feature weights.

In our case, the training dataset is composed of source code comments and their corresponding manual classification. The manually classified comments are part of a bigger dataset of \SATD comments created during ours previous studies ~\cite{Maldonado2015MTD,Maldonado2015TSE}. Basically, during these previous works, we created a public available dataset containing 62,566 comments extracted from ten open source projects. These comments were classified as \SATD comments or as regular comments (i.e., comments without technical debt). The dataset was classified by the first author and later, to mitigate the risk of bias, another student was asked to classify a statistically significant sample of the dataset. The Cohen's kappa coefficient ~\cite{cohen1960coefficient} (i.e., the level of agreement between both reviewer) was of +0.81. The resulting coefficient is scaled to range between -1 and +1, where negative value means poorer than chance agreement, zero indicates exactly chance agreement, and positive value indicates better than chance agreement ~\cite{fleiss1973equivalence}. Although we were able to find different types of \SATD in previous studies we focus our training dataset on design \SATD as it is the most common and impactful ~\cite{Ernst2015FSE} type of debt. 

Lastly, after training the NLP classifier, we process all comments that we extracted from the projects that we are analyzing. The resulting computation of this process is a classified source code comment. The classification can either be ``WHITOUT\_CLASSICATION'', meaning  that the classifier did not classify the comment as \SATD or ``DESIGN'' meaning that the classifier identified the comment as \SATD.

\subsection*{Find Technical Debt Authors}
\label{sub:find_technical_debt_authors}

Once we have classified all comments as \SATD or non \SATD we can search for the authors responsible for the introduction and eventual removal of the \SATD comments. As described in subsection \ref{sub:checkout_all_versions_of_files} we have stored all different versions of all source code files. Therefore, to find the author who introduced a \SATD comment we look into the oldest version of the file containing the analyzed comment, and incrementally search through all future versions of that file until we find the first time that the comment appears for the first time. Once found, we keep tracking the \SATD comment in the remaining future versions of the file checking if  the \SATD comment was removed. 

Additionally, when searching for authors who removed \SATD comments we have to take into consideration deleted files. As we also identify the files that once was removed we can check if the last version of the file before removal contained \SATD comments. If that is the case, the author who removed the file is also the author who removed the \SATD comments. 
